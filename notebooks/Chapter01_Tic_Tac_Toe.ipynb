{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Status\u001b[22m\u001b[39m `/mnt/E4E0A9C0E0A998F6/github/ReinforcementLearningAnIntroduction.jl/notebooks/Project.toml`\n",
      " \u001b[90m [31c24e10]\u001b[39m\u001b[37m Distributions v0.22.4\u001b[39m\n",
      " \u001b[90m [91a5bcdd]\u001b[39m\u001b[37m Plots v0.28.4\u001b[39m\n",
      " \u001b[90m [02c1da58]\u001b[39m\u001b[37m RLIntro v0.2.0 [`..`]\u001b[39m\n",
      " \u001b[90m [e575027e]\u001b[39m\u001b[37m ReinforcementLearningBase v0.5.0 [`~/workspace/github/ReinforcementLearningBase.jl`]\u001b[39m\n",
      " \u001b[90m [de1b191a]\u001b[39m\u001b[37m ReinforcementLearningCore v0.1.0 [`~/workspace/github/ReinforcementLearningCore`]\u001b[39m\n",
      " \u001b[90m [2913bbd2]\u001b[39m\u001b[37m StatsBase v0.32.0\u001b[39m\n",
      " \u001b[90m [f3b207a7]\u001b[39m\u001b[37m StatsPlots v0.12.0\u001b[39m\n",
      " \u001b[90m [2f01184e]\u001b[39m\u001b[37m SparseArrays \u001b[39m\n"
     ]
    }
   ],
   "source": [
    "]st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "┌ Info: Precompiling ReinforcementLearningCore [de1b191a-4ae0-4afa-a27b-92d07f46b2d6]\n",
      "└ @ Base loading.jl:1273\n",
      "┌ Info: Precompiling RLIntro [02c1da58-b9a1-11e8-0212-f9611b8fe936]\n",
      "└ @ Base loading.jl:1273\n",
      "┌ Warning: Package RLIntro does not have Flux in its dependencies:\n",
      "│ - If you have RLIntro checked out for development and have\n",
      "│   added Flux as a dependency but haven't updated your primary\n",
      "│   environment's manifest file, try `Pkg.resolve()`.\n",
      "│ - Otherwise you may need to report an issue with RLIntro\n",
      "└ Loading Flux into RLIntro from project dependency, future warnings for RLIntro are suppressed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "___\n",
       "___\n",
       "___\n",
       "isdone = [false], winner = [nothing]\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using ReinforcementLearningCore, RLIntro\n",
    "using RLIntro.TicTacToe\n",
    "\n",
    "env = TicTacToeEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "X"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_current_player(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5478, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observation_space, action_space = get_observation_space(env), get_action_space(env)\n",
    "nstates, nactions = length(observation_space), length(action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you are curious why there are `5478` states, you may see the discussions [here](https://math.stackexchange.com/questions/485752/tictactoe-state-space-choose-calculation/485852)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(reward = 0.0, terminal = false, state = 4193, legal_actions_mask = Bool[1, 1, 1, 1, 1, 1, 1, 1, 1, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "observe(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the Monte Carlo based method to estimate the value of each state for each player. Think about this, if we have the precise estimation of each state after taking some specific observation according to current observation, then we can just choose the action which leads to the maximum estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can create a table for each player first. By default we can set the estimations of all the states to `0.0`. Usually it won't be a problem, but here we can initialize it with a better starting point. For each state, we can check that if the state is a final state or not and set the initial estimation accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "init_table (generic function with 1 method)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function init_table(role)\n",
    "    table = zeros(nstates)\n",
    "    for i in 1:nstates\n",
    "        s = TicTacToe.ID2STATE[i]\n",
    "        isdone, winner = TicTacToe.STATES_INFO[s]\n",
    "        if isdone\n",
    "            if winner === nothing\n",
    "                table[i] = 0.5\n",
    "            elseif winner === role\n",
    "                table[i] = 1.\n",
    "            else\n",
    "                table[i] = 0.\n",
    "            end\n",
    "        else\n",
    "            table[i] = 0.5\n",
    "        end\n",
    "    end\n",
    "    table\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we wrap the table in a `TabularApproximator`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "V1 = TabularApproximator(init_table(TicTacToe.offensive));\n",
    "V2 = TabularApproximator(init_table(TicTacToe.defensive));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we construct a `MonteCarloLearner` for each player. Here the `MonteCarloLearner` is just a wrapper around the approximator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MonteCarloLearner{RLIntro.EveryVisit,TabularApproximator{1,Array{Float64,1}},CachedSampleAvg{Float64},RLIntro.NoSampling}(TabularApproximator{1,Array{Float64,1}}([0.5, 1.0, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5  …  0.5, 0.5, 0.5, 0.5, 0.0, 0.5, 0.5, 0.5, 0.5, 0.5]), 1.0, 0.1, CachedSampleAvg{Float64}(Dict{Float64,SampleAvg}()))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learner_1 = MonteCarloLearner(;approximator=V1, α=0.1, kind=EVERY_VISIT)\n",
    "learner_2 = MonteCarloLearner(;approximator=V2, α=0.1, kind=EVERY_VISIT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will create the `MonteCarloAgent`. To create such an agent, we need to provide a `learner` and a `policy`. We already have the learners above. Now let's create a policy.\n",
    "\n",
    "A policy is a mapping from states to actions. Considering that we already have the estimations of states, a simple policy would be checking the estimation of the following up states and select one action which will result to the best state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "create_mapping (generic function with 1 method)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function create_mapping(role)\n",
    "    (obs, value_learner) -> begin\n",
    "        mask = get_legal_actions_mask(obs)\n",
    "        [\n",
    "            mask[a] ? value_learner(StateOverriddenObs(obs=obs, state=TicTacToe.get_next_state_id(get_state(obs), role, a))) : 0.  # a dummy value     \n",
    "            for a in action_space\n",
    "        ]\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "ϵ = 0.01\n",
    "\n",
    "π_1 = VBasedPolicy(\n",
    "    value_learner = learner_1,\n",
    "    mapping = create_mapping(TicTacToe.offensive),\n",
    "    explorer = EpsilonGreedyExplorer(ϵ),\n",
    "    )\n",
    "\n",
    "π_2 = VBasedPolicy(\n",
    "    value_learner = learner_2,\n",
    "    mapping = create_mapping(TicTacToe.defensive),\n",
    "    explorer = EpsilonGreedyExplorer(ϵ),\n",
    "    );\n",
    "\n",
    "agent_1 = Agent(\n",
    "    policy = π_1,\n",
    "    trajectory = EpisodicCompactSARTSATrajectory(),\n",
    "    role=TicTacToe.offensive\n",
    "    );\n",
    "\n",
    "agent_2 = Agent(\n",
    "    policy = π_2,\n",
    "    trajectory = EpisodicCompactSARTSATrajectory(),\n",
    "    role=TicTacToe.defensive\n",
    "    );\n",
    "\n",
    "reset!(env)\n",
    "\n",
    "agents = (agent_1, agent_2);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32mProgress: 100%|█████████████████████████████████████████| Time: 0:00:51\u001b[39mm46\u001b[39m\n"
     ]
    }
   ],
   "source": [
    "run((agent_1, agent_2), env, StopAfterEpisode(1000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_1.policy.explorer.ϵ_stable = 0.0\n",
    "agent_2.policy.explorer.ϵ_stable = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's your turn to play this game!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "play (generic function with 1 method)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function read_action_from_stdin()\n",
    "    print(\"Your input:\")\n",
    "    input = parse(Int, readline())\n",
    "    !in(input, 1:9) && error(\"invalid input!\")\n",
    "    input\n",
    "end\n",
    "\n",
    "function play()\n",
    "    env = TicTacToeEnv()\n",
    "    println(\"\"\"You play first!\n",
    "    1 4 7\n",
    "    2 5 8\n",
    "    3 6 9\"\"\")\n",
    "    while true\n",
    "        action = read_action_from_stdin()\n",
    "        env(action)\n",
    "        println(env)\n",
    "        obs = observe(env, TicTacToe.offensive)\n",
    "        if get_terminal(obs)\n",
    "            if get_reward(obs) == 0.5\n",
    "                println(\"Tie!\")\n",
    "            elseif get_reward(obs) == 1.0 \n",
    "                println(\"You win!\")\n",
    "            else\n",
    "                println(\"Invalid input!\")\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "\n",
    "        env(agent_2(PRE_ACT_STAGE, observe(env)))\n",
    "        println(env)\n",
    "        obs = observe(env, TicTacToe.defensive)\n",
    "        if get_terminal(obs)\n",
    "            if get_reward(obs) == 0.5\n",
    "                println(\"Tie!\")\n",
    "            elseif get_reward(obs) == 1.0 \n",
    "                println(\"Your lose!\")\n",
    "            else\n",
    "                println(\"You win!\")\n",
    "            end\n",
    "            break\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You play first!\n",
      "1 4 7\n",
      "2 5 8\n",
      "3 6 9\n",
      "Your input:stdin> 5\n",
      "___\n",
      "_X_\n",
      "___\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "__O\n",
      "_X_\n",
      "___\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "Your input:stdin> 1\n",
      "X_O\n",
      "_X_\n",
      "___\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "X_O\n",
      "_X_\n",
      "__O\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "Your input:stdin> 8\n",
      "X_O\n",
      "_XX\n",
      "__O\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "X_O\n",
      "OXX\n",
      "__O\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "Your input:stdin> 6\n",
      "X_O\n",
      "OXX\n",
      "_XO\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "XOO\n",
      "OXX\n",
      "_XO\n",
      "isdone = [false], winner = [nothing]\n",
      "\n",
      "Your input:stdin> 3\n",
      "XOO\n",
      "OXX\n",
      "XXO\n",
      "isdone = [true], winner = [nothing]\n",
      "\n",
      "Tie!\n"
     ]
    }
   ],
   "source": [
    "play()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.0",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
